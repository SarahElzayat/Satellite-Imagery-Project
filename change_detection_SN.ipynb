{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "import skimage.io as io\n",
    "\n",
    "# UNet imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "\n",
    "# Custom imports\n",
    "from utilities import *\n",
    "\n",
    "# tany\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import logging\n",
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read dataset\n",
    "# def read_images(folder_path):\n",
    "#    '''\n",
    "#     Read images from a folder and return them as a list of numpy arrays.\n",
    "#     Normalize the images\n",
    "#    '''\n",
    "#    images = []\n",
    "#    file_names = os.listdir(folder_path)\n",
    "   \n",
    "#    for file_name in file_names:\n",
    "#       # image = np.array(io.imread(os.path.join(folder_path, file_name)))/255.0\n",
    "#       # images.append(image)\n",
    "#       image = np.array(io.imread(os.path.join(folder_path, file_name)))\n",
    "#       images.append(image)\n",
    "   \n",
    "#    return images\n",
    " \n",
    "# # read dataset \n",
    "# A = read_images('dataset/trainval/A') # initial images\n",
    "# B = read_images('dataset/trainval/B') # images after a certain amount of time\n",
    "# labels = read_images('dataset/trainval/label') # ground truth images (actual change)\n",
    "\n",
    "# assert len(A) == len(B) == len(labels), \"Number of images in A, B and labels are not equal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training, validation and test sets\n",
    "# A_train, A_test, B_train, B_test, labels_train, labels_test = train_test_split(A, B, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# we can use the same function to split the training set into training and validation sets\n",
    "# A_train, A_val, B_train, B_val, labels_train, labels_val = train_test_split(A_train, B_train, labels_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show a random image in the training set\n",
    "# random_index = np.random.randint(0, len(A_train))\n",
    "\n",
    "# print(\"Random image before change\")\n",
    "# io.imshow(A_train[random_index])\n",
    "# io.show()\n",
    "\n",
    "# print(\"Random image after change\")\n",
    "# io.imshow(B_train[random_index])\n",
    "# io.show()\n",
    "\n",
    "# print(\"Change ground truth of the random image\")\n",
    "# io.imshow(labels_train[random_index])\n",
    "# io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Deep Learning Methods***\n",
    "\n",
    "1. UNet\n",
    "2. Siamese\n",
    "3. SUNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **UNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet Architecture\n",
    "1. Encoder (Contracting Path): down sampling the input image size while depth increases\n",
    "\n",
    "    Each Block:\n",
    "    - Two 3*3 Convolutional Layers zero-padded with stride=1 Each Followed by a RELU Activation\n",
    "    - Max Pooling Layer 2*2 with stride=2 (Dimension halved)(Same Depth) [â¬‡ Down Sampling] \n",
    "\n",
    "2. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First we build our convolution block\n",
    "# # A convolution block for the UNet consists of two 3x3 convolutions with ReLU activation and batch normalization\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "#         self.bn = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu = nn.ReLU()\n",
    "    \n",
    "#     def forward(self, input):\n",
    "#         # first 3x3 convolution\n",
    "#         conv_output = self.conv(input)\n",
    "#         conv_output = self.bn(conv_output)\n",
    "#         conv_output = self.relu(conv_output)\n",
    "\n",
    "#         # second  convolution\n",
    "#         conv_output = self.conv(conv_output)\n",
    "#         conv_output = self.bn(conv_output)\n",
    "#         conv_output = self.relu(conv_output)\n",
    "\n",
    "#         return conv_output\n",
    "    \n",
    "# # Next we build the encoder block\n",
    "# # The encoder block consists of 4 blocks with the following layers:\n",
    "# # 1. A convolution block\n",
    "# # 2. A max pooling layer\n",
    "# # Note: The last encoder block does not have a max pooling layer\n",
    "\n",
    "# class EncoderBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(EncoderBlock, self).__init__()\n",
    "#         self.conv_block = ConvBlock(in_channels, out_channels)\n",
    "#         self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "#     def forward(self, input):\n",
    "#         conv_output = self.conv_block(input)\n",
    "#         pool_output = self.max_pool(conv_output)\n",
    "#         return conv_output, pool_output\n",
    "    \n",
    "# # Next we build the decoder block\n",
    "# # The decoder block consists of 4 blocks with the following layers:\n",
    "# # 1. nn.ConvTranspose2d convolution (transposed convolution)\n",
    "# # 2. convoloition block\n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(DecoderBlock, self).__init__()\n",
    "#         self.up_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0, output_padding=0)\n",
    "#         self.conv_block = ConvBlock(2*out_channels, out_channels)\n",
    "    \n",
    "#     def forward(self, inputs, skip):\n",
    "#         up_conv_output = self.up_conv(inputs)\n",
    "#         up_conv_output = torch.cat([up_conv_output, skip], axis=1)\n",
    "#         up_conv_output = self.conv_block(up_conv_output)\n",
    "#         return up_conv_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the UNet model\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, n_classes):\n",
    "#         super(UNet, self).__init__()\n",
    "#         ''' Encoder '''\n",
    "#         self.encoder_block1 = EncoderBlock(n_classes, 64)\n",
    "#         self.encoder_block2 = EncoderBlock(64, 128)\n",
    "#         self.encoder_block3 = EncoderBlock(128, 256)\n",
    "#         self.encoder_block4 = EncoderBlock(256, 512)\n",
    "        \n",
    "#         ''' Bottle Neck'''\n",
    "#         self.bottleneck = ConvBlock(512, 1024)\n",
    "#         # No max pooling in the bottleneck => considered last layer in the encoder\n",
    "        \n",
    "#         ''' Decoder '''\n",
    "#         self.decoder_block4 = DecoderBlock(1024, 512)\n",
    "#         self.decoder_block3 = DecoderBlock(512, 256)\n",
    "#         self.decoder_block2 = DecoderBlock(256, 128)\n",
    "#         self.decoder_block1 = DecoderBlock(128, 64)\n",
    "\n",
    "        \n",
    "#         ''' Classifier '''\n",
    "#         self.output_conv = nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "#     def forward(self, input):\n",
    "#         # encoder\n",
    "#         e1, p1 = self.encoder_block1(input)\n",
    "#         e2, p2 = self.encoder_block2(p1)\n",
    "#         e3, p3 = self.encoder_block3(p2)\n",
    "#         e4, p4 = self.encoder_block4(p3)\n",
    "        \n",
    "#         # bottle neck\n",
    "#         bn = self.center(p4)\n",
    "        \n",
    "#         # decoder\n",
    "#         d4 = self.decoder_block4(bn, e4)\n",
    "#         d3 = self.decoder_block3(d4, e3)\n",
    "#         d2 = self.decoder_block2(d3, e2)\n",
    "#         d1 = self.decoder_block1(d2, e1)\n",
    "        \n",
    "#         # output\n",
    "#         output = self.output_conv(d1)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_per_epoch,bce_loss_per_epoch,dice_loss_per_epoch,jacord_index_per_epoch,loss_per_val_epoch,bce_loss_per_val_epoch,dice_loss_per_val_epoch,jacord_index_per_val_epoch = run(UNet, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Siamese UNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the dataset using dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET LOADED\n"
     ]
    }
   ],
   "source": [
    "class LoadDataset(Dataset):\n",
    "    def __init__(self, input_folder, transforms_list=[]):\n",
    "        \n",
    "        self.before_folder = os.path.join(input_folder, 'A')\n",
    "        self.after_folder = os.path.join(input_folder, 'B')\n",
    "        self.label_folder = os.path.join(input_folder, 'label')\n",
    "\n",
    "        self.file_names = os.listdir(self.before_folder) # any folder msh far2a\n",
    "\n",
    "        self.transforms = transforms_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        before_image = io.imread(os.path.join(self.before_folder, self.file_names[idx]))\n",
    "        after_image = io.imread(os.path.join(self.before_folder, self.file_names[idx]))\n",
    "        label = io.imread(os.path.join(self.label_folder, self.file_names[idx]))\n",
    "\n",
    "        \n",
    "        label = label.astype('float32')  # Convert to floating point to allow division\n",
    "        label = label > 0\n",
    "        label = label.astype(np.int64)\n",
    "        label = torch.as_tensor(label, dtype=torch.float32)\n",
    "        label = label.squeeze()\n",
    "\n",
    "        if len(self.transforms) == 2:\n",
    "            before_image = self.transforms[0](before_image)\n",
    "            after_image = self.transforms[1](after_image)\n",
    "\n",
    "\n",
    "        return {'images': (before_image, after_image), 'label': label}\n",
    "    \n",
    "# Define the transformations\n",
    "transform = [transforms.Compose([transforms.ToTensor()]), transforms.Compose([transforms.ToTensor()])]\n",
    "\n",
    "# Load the dataset\n",
    "dataset = LoadDataset('dataset/trainval', transform)\n",
    "\n",
    "# Split the dataset into training, test, and validation sets (80, 10, 10)\n",
    "train_set, temp_set = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "val_set, test_set = train_test_split(temp_set, test_size=0.5, random_state=42)\n",
    "\n",
    "# create the DataLoader\n",
    "dataloader = {\n",
    "    'train': DataLoader(train_set, batch_size=8, shuffle=True),\n",
    "    'val': DataLoader(val_set, batch_size=8, shuffle=False),\n",
    "    'test': DataLoader(test_set, batch_size=8, shuffle=False)\n",
    "}\n",
    "\n",
    "print(\"DATASET LOADED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Build the Siamese model\n",
    "\n",
    "<img src=\"siamese_architecture.jpg\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.padding import ReplicationPad2d\n",
    "\n",
    "class SiameseUNet(nn.Module):\n",
    "    \"\"\"SiamUnet_diff segmentation network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_nbr, label_nbr):\n",
    "        super(SiameseUNet, self).__init__()\n",
    "\n",
    "        self.input_nbr = input_nbr\n",
    "\n",
    "        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(16)\n",
    "        self.do11 = nn.Dropout2d(p=0.2)\n",
    "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(16)\n",
    "        self.do12 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(32)\n",
    "        self.do21 = nn.Dropout2d(p=0.2)\n",
    "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(32)\n",
    "        self.do22 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(64)\n",
    "        self.do31 = nn.Dropout2d(p=0.2)\n",
    "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(64)\n",
    "        self.do32 = nn.Dropout2d(p=0.2)\n",
    "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn33 = nn.BatchNorm2d(64)\n",
    "        self.do33 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(128)\n",
    "        self.do41 = nn.Dropout2d(p=0.2)\n",
    "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(128)\n",
    "        self.do42 = nn.Dropout2d(p=0.2)\n",
    "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn43 = nn.BatchNorm2d(128)\n",
    "        self.do43 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn43d = nn.BatchNorm2d(128)\n",
    "        self.do43d = nn.Dropout2d(p=0.2)\n",
    "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn42d = nn.BatchNorm2d(128)\n",
    "        self.do42d = nn.Dropout2d(p=0.2)\n",
    "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn41d = nn.BatchNorm2d(64)\n",
    "        self.do41d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn33d = nn.BatchNorm2d(64)\n",
    "        self.do33d = nn.Dropout2d(p=0.2)\n",
    "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn32d = nn.BatchNorm2d(64)\n",
    "        self.do32d = nn.Dropout2d(p=0.2)\n",
    "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn31d = nn.BatchNorm2d(32)\n",
    "        self.do31d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn22d = nn.BatchNorm2d(32)\n",
    "        self.do22d = nn.Dropout2d(p=0.2)\n",
    "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn21d = nn.BatchNorm2d(16)\n",
    "        self.do21d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn12d = nn.BatchNorm2d(16)\n",
    "        self.do12d = nn.Dropout2d(p=0.2)\n",
    "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
    "\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "\n",
    "        \"\"\"Forward method.\"\"\"\n",
    "        # for imput image 1\n",
    "        # Stage 1\n",
    "        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n",
    "        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
    "        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        # Stage 2\n",
    "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
    "        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
    "        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 3\n",
    "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
    "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
    "        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
    "        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4\n",
    "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
    "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
    "        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
    "        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n",
    "\n",
    "        ####################################################\n",
    "        # for input image 2\n",
    "        # Stage 1\n",
    "        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n",
    "        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
    "        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        # Stage 2\n",
    "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
    "        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
    "        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 3\n",
    "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
    "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
    "        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
    "        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4\n",
    "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
    "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
    "        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
    "        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "\n",
    "        # Stage 4d\n",
    "        x4d = self.upconv4(x4p)\n",
    "        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n",
    "        x4d = torch.cat((pad4(x4d), torch.abs(x43_1 - x43_2)), 1)\n",
    "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
    "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
    "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
    "\n",
    "        # Stage 3d\n",
    "        x3d = self.upconv3(x41d)\n",
    "        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n",
    "        x3d = torch.cat((pad3(x3d), torch.abs(x33_1 - x33_2)), 1)\n",
    "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
    "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
    "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
    "\n",
    "        # Stage 2d\n",
    "        x2d = self.upconv2(x31d)\n",
    "        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n",
    "        x2d = torch.cat((pad2(x2d), torch.abs(x22_1 - x22_2)), 1)\n",
    "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
    "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
    "\n",
    "        # Stage 1d\n",
    "        x1d = self.upconv1(x21d)\n",
    "        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n",
    "        x1d = torch.cat((pad1(x1d), torch.abs(x12_1 - x12_2)), 1)\n",
    "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
    "        x11d = self.conv11d(x12d)\n",
    "\n",
    "        return self.sm(x11d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "# criterion = nn.NLLLoss()\n",
    "# model = SiameseUNet(3, 2) # 3 input channels, 2 classes\n",
    "\n",
    "# model.cuda()\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
    "\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#     model.train()\n",
    "\n",
    "#     print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
    "#     print('-' * 10)\n",
    "\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for batch in dataloader['train']:\n",
    "\n",
    "#         # load the data to the device\n",
    "#         before_image = batch[0].to(device)\n",
    "#         after_image = batch[1].to(device)\n",
    "#         label = batch[2].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         output = model(before_image, after_image)\n",
    "\n",
    "\n",
    "#         loss = criterion(output, label.long())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "        \n",
    "#         # Calculate accuracy\n",
    "#         _, predicted_indices = torch.max(output.data, 1)\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         predicted_indices = predicted_indices.int().cpu().numpy()\n",
    "\n",
    "#         label_np = label.cpu().numpy()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jaccard_indices=[]\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# for batch in dataloader['test']:\n",
    "#     before_image = batch[0].to(device)\n",
    "#     after_image = batch[1].to(device)\n",
    "#     label = batch[2].to(device)\n",
    "\n",
    "#     output = model(before_image, after_image)\n",
    "\n",
    "#     _, predicted_indices = torch.max(output.data, 1)\n",
    "\n",
    "#     predicted_indices = predicted_indices.int().cpu().numpy()\n",
    "\n",
    "#     label_np = label.cpu().numpy()\n",
    "\n",
    "    \n",
    "#     for i in range(predicted_indices.shape[0]):\n",
    "#         cv2.imwrite(\"/content/images/\"+f'{i}'+\".jpg\", predicted_indices[i].reshape(256 , 256, 1)*255)\n",
    "#         cv2.imwrite(\"/content/labels/\"+f'{i}'+\".jpg\", label_np[i].reshape(256 , 256, 1)*255)\n",
    "\n",
    "#     jaccard_indices.append(jaccard_score(label_np.flatten(), predicted_indices.flatten(), zero_division=1))\n",
    "    \n",
    "\n",
    "# avg_jaccard_index_sklearn = np.mean(jaccard_indices)*100\n",
    "\n",
    "# print(\"AVERAGE JACCARD INDEX sklearn + \",avg_jaccard_index_sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Siamese UNet ECAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    " \n",
    "# The convolution block architecture consists of:\n",
    "# 1. Convolution layer with kernel size 3x3 and padding 1 (in_channels, mid_channel)\n",
    "# 2. Batch normalization\n",
    "# 3. ReLU activation\n",
    "# 4. Second convolution layer with kernel size 3x3 and padding 1 (mid_channel, out_channels)\n",
    "# 5. Batch normalization\n",
    "# 6. ReLU activation of the fist convolution layer with the output from second batch normalization\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channel, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channel, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channel)\n",
    "        self.conv2 = nn.Conv2d(mid_channel, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True) # activation function (inplace modifies input directly)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = self.conv1(input) # first convolution layer\n",
    "\n",
    "        # save the result of the first convolution for the last layer\n",
    "        x = input\n",
    "\n",
    "        input = self.bn1(input) # first batch normalization\n",
    "        input = self.relu(input) # activation function\n",
    "\n",
    "        input = self.conv2(input) # second convolution layer\n",
    "        input = self.bn2(input)\n",
    "\n",
    "        # add the result of the first convolution to the output of the second convolution\n",
    "        input += x\n",
    "        output = self.relu(input) # final activation function\n",
    "        return output\n",
    "\n",
    "\n",
    "# The channel attention module\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio = 16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels,in_channels//ratio,1,bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_channels//ratio, in_channels,1,bias=False)\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmod(out)\n",
    "    \n",
    "\n",
    "# cuild the model\n",
    "class SiameseUNetECAM(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(SiameseUNetECAM, self).__init__()\n",
    "        torch.nn.Module.dump_patches = True # enables a feature in PyTorch where any changes to the module hierarchy are tracked and patches are dumped to files.\n",
    "\n",
    "        n1 = 32     # the initial number of channels of feature map\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv0_0 = ConvBlock(input_channels, filters[0], filters[0])\n",
    "        self.conv1_0 = ConvBlock(filters[0], filters[1], filters[1])\n",
    "\n",
    "        self.Up1_0 = nn.ConvTranspose2d(filters[1], filters[1], 2, stride=2)\n",
    "\n",
    "        self.conv2_0 = ConvBlock(filters[1], filters[2], filters[2])\n",
    "\n",
    "        self.Up2_0 = nn.ConvTranspose2d(filters[2], filters[2], 2, stride=2)\n",
    "\n",
    "        self.conv3_0 = ConvBlock(filters[2], filters[3], filters[3])\n",
    "\n",
    "        self.Up3_0 = nn.ConvTranspose2d(filters[3], filters[3], 2, stride=2)\n",
    "        self.conv4_0 = ConvBlock(filters[3], filters[4], filters[4])\n",
    "\n",
    "        self.Up4_0 = nn.ConvTranspose2d(filters[4], filters[4], 2, stride=2)\n",
    "\n",
    "        self.conv0_1 = ConvBlock(filters[0] * 2 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_1 = ConvBlock(filters[1] * 2 + filters[2], filters[1], filters[1])\n",
    "        self.Up1_1 = nn.ConvTranspose2d(filters[1], filters[1], 2, stride=2)\n",
    "        self.conv2_1 = ConvBlock(filters[2] * 2 + filters[3], filters[2], filters[2])\n",
    "        self.Up2_1 = nn.ConvTranspose2d(filters[2], filters[2], 2, stride=2)\n",
    "        self.conv3_1 = ConvBlock(filters[3] * 2 + filters[4], filters[3], filters[3])\n",
    "        self.Up3_1 = nn.ConvTranspose2d(filters[3], filters[3], 2, stride=2)\n",
    "\n",
    "        self.conv0_2 = ConvBlock(filters[0] * 3 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_2 = ConvBlock(filters[1] * 3 + filters[2], filters[1], filters[1])\n",
    "        self.Up1_2 = nn.ConvTranspose2d(filters[1], filters[1], 2, stride=2)\n",
    "        self.conv2_2 = ConvBlock(filters[2] * 3 + filters[3], filters[2], filters[2])\n",
    "        self.Up2_2 = nn.ConvTranspose2d(filters[2], filters[2], 2, stride=2)\n",
    "\n",
    "        self.conv0_3 = ConvBlock(filters[0] * 4 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_3 = ConvBlock(filters[1] * 4 + filters[2], filters[1], filters[1])\n",
    "        self.Up1_3 = nn.ConvTranspose2d(filters[1], filters[1], 2, stride=2)\n",
    "\n",
    "        self.conv0_4 = ConvBlock(filters[0] * 5 + filters[1], filters[0], filters[0])\n",
    "\n",
    "        self.ca = ChannelAttention(filters[0] * 4, ratio=16)\n",
    "        self.ca1 = ChannelAttention(filters[0], ratio=16 // 4)\n",
    "\n",
    "        self.conv_final = nn.Conv2d(filters[0] * 4, output_channels, kernel_size=1)\n",
    "\n",
    "        # msh fahma dy beta3mel eh bas mashy ba3deen\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, xA, xB):\n",
    "        '''xA'''\n",
    "        x0_0A = self.conv0_0(xA)\n",
    "        x1_0A = self.conv1_0(self.pool(x0_0A))\n",
    "        x2_0A = self.conv2_0(self.pool(x1_0A))\n",
    "        x3_0A = self.conv3_0(self.pool(x2_0A))\n",
    "        # x4_0A = self.conv4_0(self.pool(x3_0A))\n",
    "        '''xB'''\n",
    "        x0_0B = self.conv0_0(xB)\n",
    "        x1_0B = self.conv1_0(self.pool(x0_0B))\n",
    "        x2_0B = self.conv2_0(self.pool(x1_0B))\n",
    "        x3_0B = self.conv3_0(self.pool(x2_0B))\n",
    "        x4_0B = self.conv4_0(self.pool(x3_0B))\n",
    "\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0A, x0_0B, self.Up1_0(x1_0B)], 1))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0A, x1_0B, self.Up2_0(x2_0B)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0A, x0_0B, x0_1, self.Up1_1(x1_1)], 1))\n",
    "\n",
    "\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0A, x2_0B, self.Up3_0(x3_0B)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0A, x1_0B, x1_1, self.Up2_1(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0A, x0_0B, x0_1, x0_2, self.Up1_2(x1_2)], 1))\n",
    "\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0A, x3_0B, self.Up4_0(x4_0B)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0A, x2_0B, x2_1, self.Up3_1(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0A, x1_0B, x1_1, x1_2, self.Up2_2(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0A, x0_0B, x0_1, x0_2, x0_3, self.Up1_3(x1_3)], 1))\n",
    "\n",
    "        output = torch.cat([x0_1, x0_2, x0_3, x0_4], 1)\n",
    "\n",
    "        intra = torch.sum(torch.stack((x0_1, x0_2, x0_3, x0_4)), dim=0)\n",
    "        ca1 = self.ca1(intra)\n",
    "        output = self.ca(output) * (output + ca1.repeat(1, 4, 1, 1))\n",
    "        output = self.conv_final(output)\n",
    "\n",
    "        return (output, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some functions and definitions for training\n",
    "parameters = {\n",
    "  \"patch_size\": 256,\n",
    "  \"num_gpus\": 1,\n",
    "  \"num_workers\": 8,\n",
    "  \"num_channel\": 3,\n",
    "  \"epochs\": 10,\n",
    "  \"batch_size\": 8,\n",
    "  \"learning_rate\": 1e-3,\n",
    "  \"loss_function\": \"hybrid\",\n",
    "  \"dataset_dir\": \"./dataset/trainval/\",\n",
    "  \"weight_dir\": \"./content/\",\n",
    "  \"log_dir\": \"./log/\"\n",
    "}\n",
    "\n",
    "train_set = dataloader['train']\n",
    "val_set = dataloader['val']\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def initialize_metrics():\n",
    "    \"\"\"Generates a dictionary of metrics with metrics as keys\n",
    "       and empty lists as values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        a dictionary of metrics\n",
    "\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'cd_losses': [],\n",
    "        'cd_corrects': [],\n",
    "        'cd_precisions': [],\n",
    "        'cd_recalls': [],\n",
    "        'cd_f1scores': [],\n",
    "        'learning_rate': [],\n",
    "        'jaccard_scores': []\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def set_metrics(metric_dict, cd_loss, cd_corrects, cd_report, lr, jaccard_score):\n",
    "    \"\"\"Updates metric dict with batch metrics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metric_dict : dict\n",
    "        dict of metrics\n",
    "    cd_loss : dict(?)\n",
    "        loss value\n",
    "    cd_corrects : dict(?)\n",
    "        number of correct results (to generate accuracy\n",
    "    cd_report : list\n",
    "        precision, recall, f1 values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        dict of  updated metrics\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    metric_dict['cd_losses'].append(cd_loss.item())\n",
    "    metric_dict['cd_corrects'].append(cd_corrects.item())\n",
    "    metric_dict['cd_precisions'].append(cd_report[0])\n",
    "    metric_dict['cd_recalls'].append(cd_report[1])\n",
    "    metric_dict['cd_f1scores'].append(cd_report[2])\n",
    "    metric_dict['learning_rate'].append(lr)\n",
    "    metric_dict['jaccard_scores'].append(jaccard_score)\n",
    "\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_mean_metrics(metric_dict):\n",
    "    \"\"\"takes a dictionary of lists for metrics and returns dict of mean values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metric_dict : dict\n",
    "        A dictionary of metrics\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        dict of floats that reflect mean metric value\n",
    "\n",
    "    \"\"\"\n",
    "    return {k: np.mean(v) for k, v in metric_dict.items()}\n",
    "\n",
    "\n",
    "def hybrid_loss(predictions, target, device):\n",
    "    \"\"\"Calculating the loss\"\"\"\n",
    "    loss = 0\n",
    "\n",
    "    # gamma=0, alpha=None --> CE\n",
    "    focal = metrics.FocalLoss(gamma=0, alpha=None)\n",
    "\n",
    "    for prediction in predictions:\n",
    "\n",
    "        bce = focal(prediction, target)\n",
    "        dice = metrics.dice_loss(prediction, target, device)\n",
    "        loss += bce + dice\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:GPU AVAILABLE? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:LOADING Model\n",
      "INFO:root:STARTING training\n",
      "INFO:root:SET model mode to train!\n",
      "epoch 0 info 0 - 8:   0%|          | 0/426 [00:00<?, ?it/s]c:\\Users\\yazmi\\OneDrive\\Desktop\\Uni\\Fourth Year\\Second Semester\\Remote Sensing and Satellite Imagery\\Project\\Satellite-Imagery-Project\\metrics.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logpt = F.log_softmax(input)\n",
      "epoch 0 info 0 - 8:   0%|          | 0/426 [02:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m loss \u001b[38;5;241m=\u001b[39m cd_loss\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     88\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yazmi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Initialize experiments log\n",
    "\"\"\"\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "writer = SummaryWriter(parameters['log_dir'] + f'/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}/')\n",
    "\n",
    "\"\"\"\n",
    "Set up environment: define paths, download data, and set device\n",
    "\"\"\"\n",
    "logging.info('GPU AVAILABLE? ' + str(torch.cuda.is_available()))\n",
    "\n",
    "seed_torch(seed=777)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load Model then define other aspects of the model\n",
    "\"\"\"\n",
    "logging.info('LOADING Model')\n",
    "model = SiameseUNetECAM(3, 2).to(device)\n",
    "\n",
    "criterion = hybrid_loss # loss function bce + dice\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=parameters['learning_rate']) # Be careful when you adjust learning rate, you can refer to the linear scaling rule\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.5)\n",
    "\n",
    "\"\"\"\n",
    " Set starting values\n",
    "\"\"\"\n",
    "best_metrics = {'cd_f1scores': -1, 'cd_recalls': -1, 'cd_precisions': -1}\n",
    "logging.info('STARTING training')\n",
    "total_step = -1\n",
    "\n",
    "\n",
    "loss_per_epoch=[]\n",
    "bce_loss_per_epoch=[]\n",
    "dice_loss_per_epoch=[]\n",
    "jacord_index_per_epoch=[]\n",
    "\n",
    "loss_per_val_epoch=[]\n",
    "bce_loss_per_val_epoch=[]\n",
    "dice_loss_per_val_epoch=[]\n",
    "jacord_index_per_val_epoch=[]\n",
    "\n",
    "# training loop\n",
    "for epoch in range(parameters['epochs']):\n",
    "    train_metrics = initialize_metrics()\n",
    "    val_metrics = initialize_metrics()\n",
    "\n",
    "    lab_metrics = defaultdict(float)\n",
    "\n",
    "    \"\"\"\n",
    "    Begin Training\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    logging.info('SET model mode to train!')\n",
    "\n",
    "    batch_iteration = 0\n",
    "\n",
    "    tbar = tqdm(train_set)\n",
    "    for batch in tbar:\n",
    "        tbar.set_description(\"epoch {} info \".format(epoch) + str(batch_iteration) + \" - \" + str(batch_iteration + parameters['batch_size']))\n",
    "        batch_iteration = batch_iteration + parameters['batch_size']\n",
    "        total_step += 1\n",
    "\n",
    "        # load the data to the device\n",
    "        before_images = batch['images'][0].to(device)\n",
    "        after_images = batch['images'][1].to(device)\n",
    "        labels = batch['label'].long().to(device)\n",
    "\n",
    "        \n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get model predictions, calculate loss, backprop\n",
    "        predictions = model(before_images, after_images)\n",
    "\n",
    "        # calculate the loss\n",
    "        cd_loss = criterion(predictions, labels, device)\n",
    "        loss = cd_loss\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        predictions = predictions[-1]\n",
    "        _, predictions = torch.max(predictions, 1)\n",
    "\n",
    "        # evaluation and metrics\n",
    "        jac_score = jaccard_score(labels.data.cpu().numpy().flatten(),\n",
    "                                predictions.data.cpu().numpy().flatten(), \n",
    "                                zero_division=1)\n",
    "\n",
    "        cd_corrects = (100 *\n",
    "                       (predictions.squeeze().byte() == labels.squeeze().byte()).sum() /\n",
    "                       (labels.size()[0] * (parameters['patch_size']**2)))\n",
    "\n",
    "        cd_train_report = prfs(labels.data.cpu().numpy().flatten(),\n",
    "                               predictions.data.cpu().numpy().flatten(),\n",
    "                               average='binary',\n",
    "                               zero_division=0,\n",
    "                               pos_label=1)\n",
    "\n",
    "        train_metrics = set_metrics(train_metrics,\n",
    "                                    cd_loss,\n",
    "                                    cd_corrects,\n",
    "                                    cd_train_report,\n",
    "                                    scheduler.get_last_lr(),\n",
    "                                    jac_score)\n",
    "\n",
    "        # log the batch mean metrics\n",
    "        mean_train_metrics = get_mean_metrics(train_metrics)\n",
    "\n",
    "        for k, v in mean_train_metrics.items():\n",
    "            writer.add_scalars(str(k), {'train': v}, total_step)\n",
    "\n",
    "        # clear batch variables from memory\n",
    "        del before_images, after_images, labels\n",
    "    \n",
    "    scheduler.step()\n",
    "    logging.info(\"EPOCH {} TRAIN METRICS\".format(epoch) + str(mean_train_metrics))\n",
    "\n",
    "    print('An epoch finished.')\n",
    "    \n",
    "    \n",
    "writer.close()  # close tensor board\n",
    "print('Done!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
